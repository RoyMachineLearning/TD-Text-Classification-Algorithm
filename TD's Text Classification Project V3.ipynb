{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries for NLTK\n",
    "from nltk.corpus import stopwords, gazetteers\n",
    "from nltk import sent_tokenize, wordpunct_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#Libraries for Text Processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# Libraries for Evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "#Models\n",
    "from sklearn.ensemble import RandomForestClassifier     # Random Forest\n",
    "from sklearn.naive_bayes import MultinomialNB           # Nave Bayes\n",
    "from sklearn.linear_model import LogisticRegression     # logistic regression\n",
    "from sklearn.tree import DecisionTreeClassifier         \n",
    "from sklearn.ensemble import AdaBoostClassifier         # Ada Boost Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier # Gradient Boosting\n",
    "from sklearn.ensemble import ExtraTreesClassifier       # Extra Trees Classifier\n",
    "from sklearn.ensemble import BaggingClassifier          # Bagging Classifier\n",
    "from sklearn.ensemble import VotingClassifier           # Ensemble Model\n",
    "import tensorflow as TF                                 # Deep Neural Networks\n",
    "\n",
    "# Other Libraries\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(123)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "# Cross-validation\n",
    "from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score #score evaluation\n",
    "from sklearn.model_selection import cross_val_predict #prediction\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_ROOT = \"C:\\\\Users\\\\Owner\\\\Desktop\\\\TD Text Analytics\\\\\"\n",
    "TD_SOURCE_FILE = FILE_ROOT + \"train.txt\"\n",
    "LABEL_FILE = FILE_ROOT + \"labels_Candidate.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this module only once - NLTK Stopwords \n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('gazetteers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing data - removing non ascii characters, Conver the words to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_non_ascii(string):\n",
    "    ''' Returns the string without non ASCII characters'''\n",
    "    stripped = (c for c in string if 0 < ord(c) < 127)\n",
    "    return ''.join(stripped)\n",
    "\n",
    "def pre_process(text):\n",
    "            \n",
    "        #Remove non-ascii characters\n",
    "        text = strip_non_ascii(text)\n",
    "                \n",
    "        #Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        #Convert words with repeated characters\n",
    "        text = re.sub(r'([a-z])\\1{2,}',r'\\1',text)\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = [word for sent in sent_tokenize(text) for word in wordpunct_tokenize(sent)]\n",
    "         \n",
    "    #Remove stopwords\n",
    "    stop = stopwords.words(\"english\")\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "\n",
    "    #Remove all words less than three chars\n",
    "    tokens = [token for token in tokens if len(token) >= 3]\n",
    "        \n",
    "    #Apply stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens  = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the Data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1244,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFile = pd.read_csv(TD_SOURCE_FILE, sep = None)\n",
    "\n",
    "DataFile.columns = [\"ID\", \"Wd\",\"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will merge all the words for every comment, to form complete sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1245,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewDataFile = DataFile.groupby('ID').apply(lambda x: x['Text'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will replace my lists to Strings for text analysis. This will also eliminate the brakets \"[]\" and the comma separator from the entire text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1246,
   "metadata": {},
   "outputs": [],
   "source": [
    "FormatDataFile = pd.DataFrame(NewDataFile.apply(lambda x:' '.join(x)))\n",
    "FormatDataFile.columns = [\"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Partition the data to identify the initial training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now split Back The data to training and test set - before applying the pipeline\n",
    "train_set, test_set = train_test_split(FormatDataFile, \n",
    "                                       test_size=0.28205,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3998, 1)"
      ]
     },
     "execution_count": 1248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1572, 1)"
      ]
     },
     "execution_count": 1249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can now add the labels as a new column to the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1250,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = pd.read_csv(LABEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3999, 1)"
      ]
     },
     "execution_count": 1251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the training labels\n",
    "Extract_Train_Set = Labels[0:3999]\n",
    "Extract_Train_Set.columns = [\"ID\", \"Label\"]\n",
    "Extract_Train_Set = Extract_Train_Set.drop(\"ID\",axis=1)\n",
    "\n",
    "# Get the first column for test set\n",
    "Extract_Text_Label = Labels[4000:]\n",
    "Extract_Text_Label.columns = [\"ID\", \"Label\"]\n",
    "Extract_Text_Label = Extract_Text_Label.drop(\"Label\",axis=1)\n",
    "\n",
    "Extract_Train_Set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1252,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['Label'] = Extract_Train_Set['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3998, 2)"
      ]
     },
     "execution_count": 1253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Training and Test Sets already defined and finalized, We also need a validation set to tune the model. Since we dont have test labels, Validation set approach will help improving over the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now split Back The training data to training and validation set - before applying the pipeline\n",
    "training_set, validation_set = train_test_split(train_set, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define x and y.\n",
    "\n",
    "#the Y Variable\n",
    "train_set_y = training_set[\"Label\"].copy()\n",
    "validation_set_y = validation_set[\"Label\"].copy()\n",
    "\n",
    "#the X variables\n",
    "train_set_X = training_set.drop(\"Label\", axis=1)\n",
    "validation_set_X = validation_set.drop(\"Label\", axis=1)\n",
    "test_set_X = test_set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the TDM using the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1289,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', preprocessor=pre_process, tokenizer=tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1290,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit(train_set_X[\"Text\"])\n",
    "x_train_count = cv.transform(train_set_X[\"Text\"])\n",
    "x_validation_count = cv.transform(validation_set_X[\"Text\"])\n",
    "x_test_count = cv.transform(test_set_X[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1291,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}',preprocessor=pre_process,tokenizer=tokenize)\n",
    "\n",
    "tfidf_vect.fit(train_set_X[\"Text\"])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_set_X[\"Text\"])\n",
    "xvalid_tfidf =  tfidf_vect.transform(validation_set_X[\"Text\"])\n",
    "xtest_tfidf =   tfidf_vect.transform(test_set_X[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1292,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tdm = x_train_count.toarray()\n",
    "x_validation_tdm = x_validation_count.toarray()\n",
    "x_test_tdm = x_test_count.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3198, 6166)"
      ]
     },
     "execution_count": 1293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tdm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Term Document Matrix has 29544 features. The next step is to reduce these dimensions and extract the best 1000 features using chi2 test statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1294,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch2 = SelectKBest(chi2, k=1000)\n",
    "x_train_tdm = ch2.fit_transform(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1295,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_tdm = ch2.transform(x_validation_tdm)\n",
    "x_test_tdm = ch2.transform(x_test_tdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the first model - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    8.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [5, 7, 10], 'min_samples_split': [0.3, 0.5, 2], 'min_samples_leaf': [0.1, 0.5, 1]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 1300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_class = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [5, 7, 10]\n",
    "min_samples_split = [0.3,0.5,2]\n",
    "min_samples_leaf = [0.1,0.5,1]\n",
    "\n",
    "param_grid_forest = {'n_estimators' : n_estimators, \n",
    "                     'min_samples_split' : min_samples_split,\n",
    "                     'min_samples_leaf': min_samples_leaf }\n",
    "\n",
    "\n",
    "rand_search_forest = RandomizedSearchCV(forest_class, param_grid_forest, cv = 4, scoring='roc_auc', refit = True,\n",
    "                                 n_jobs = -1, verbose=2)\n",
    "\n",
    "rand_search_forest.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 5, 'min_samples_split': 0.3, 'min_samples_leaf': 1}\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=0.3,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# Now we will try searching the best estimator and predict the values on the training set\n",
    "\n",
    "forest_best_params_ = rand_search_forest.best_params_\n",
    "forest_best_estimators_ = rand_search_forest.best_estimator_\n",
    "\n",
    "print(forest_best_params_)\n",
    "print(forest_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9143214509068167"
      ]
     },
     "execution_count": 1302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the training accuracy with Random Forest\n",
    "\n",
    "y_pred_forest = forest_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87625"
      ]
     },
     "execution_count": 1303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_forest_validation = forest_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_forest_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_forest_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we notice, Random Forest fits well on the validation set with 87.6% accuracy. The model generalizes well. but before making a final conclusion that lets build more models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nave Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 3 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    4.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    4.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'alpha': [0.0001, 0.001, 1]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 1304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_Bayes_ = MultinomialNB()\n",
    "\n",
    "alpha = [0.0001, 0.001, 1]\n",
    "\n",
    "param_grid_Bayes = {'alpha':alpha}\n",
    "\n",
    "grid_search_bayes = GridSearchCV(N_Bayes_, param_grid_Bayes, cv = 4, scoring='roc_auc', refit = True,\n",
    "                                 n_jobs = -1, verbose=2)\n",
    "\n",
    "grid_search_bayes.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1}\n",
      "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)\n"
     ]
    }
   ],
   "source": [
    "bayes_best_params_ = grid_search_bayes.best_params_\n",
    "bayes_best_estimators_ = grid_search_bayes.best_estimator_\n",
    "\n",
    "print(bayes_best_params_)\n",
    "print(bayes_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9036898061288305"
      ]
     },
     "execution_count": 1306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the training accuracy with Naive Bayes\n",
    "\n",
    "y_pred_bayes = bayes_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8875"
      ]
     },
     "execution_count": 1307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_bayes_validation = bayes_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_bayes_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results shows that Naive Bayes model fits well with a Validation accuracy rate of 88.75%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 1308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state = 42)\n",
    "log_reg.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 3 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    4.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    4.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'C': [0.0001, 0.001, 1]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 1310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state = 42)\n",
    "\n",
    "C = [0.0001, 0.001, 1]\n",
    "                            \n",
    "param_grid_log_reg = {'C' : C}\n",
    "\n",
    "rand_search_log_reg = GridSearchCV(log_reg, param_grid_log_reg, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_log_reg.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1}\n",
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "logreg_best_params_ = rand_search_log_reg.best_params_\n",
    "logreg_best_estimators_ = rand_search_log_reg.best_estimator_\n",
    "\n",
    "print(logreg_best_params_)\n",
    "print(logreg_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8899312070043778"
      ]
     },
     "execution_count": 1312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the training accuracy with Logistic Regression\n",
    "\n",
    "y_pred_logreg = logreg_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88875"
      ]
     },
     "execution_count": 1313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_logreg_validation = logreg_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_logreg_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model performs well on the validation set with 88.8% accuracy on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ADA Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   26.4s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   31.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=42),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [5, 50], 'learning_rate': [0.001, 0.01, 1], 'algorithm': ['SAMME', 'SAMME.R']},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 1404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_boost = AdaBoostClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [5, 50]\n",
    "learning_rate = [0.001, 0.01, 1]\n",
    "algorithm = ['SAMME', 'SAMME.R']\n",
    "\n",
    "param_grid_ada = {'n_estimators' : n_estimators, 'learning_rate' : learning_rate, 'algorithm' : algorithm}\n",
    "\n",
    "rand_search_ada = RandomizedSearchCV(ada_boost, param_grid_ada, cv = 4, scoring='roc_auc', refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_ada.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 50, 'learning_rate': 1, 'algorithm': 'SAMME.R'}\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,\n",
      "          n_estimators=50, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "ada_best_params_ = rand_search_ada.best_params_\n",
    "ada_best_estimators_ = rand_search_ada.best_estimator_\n",
    "\n",
    "print(ada_best_params_)\n",
    "print(ada_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8655409631019387"
      ]
     },
     "execution_count": 1406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ada_estimator = ada_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_ada_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88625"
      ]
     },
     "execution_count": 1407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_ada_validation = ada_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_ada_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ADA Classifier Model performs better with validation set as the accuracy improves to 88.6%, showing signs of underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:  4.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [50, 100], 'learning_rate': [0.1, 0.5], 'max_depth': [3, 5], 'min_samples_split': [2, 4], 'min_samples_leaf': [1, 5]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 1410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB_Classifier = GradientBoostingClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [50, 100]\n",
    "learning_rate = [0.1, 0.5]\n",
    "max_depth = [3,5]\n",
    "min_samples_split = [2, 4]\n",
    "min_samples_leaf = [1, 5]\n",
    "                            \n",
    "param_grid_grad_boost = {'n_estimators' : n_estimators, 'learning_rate' : learning_rate,\n",
    "                              'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n",
    "                              'min_samples_leaf' : min_samples_leaf}\n",
    "\n",
    "rand_search_grad_boost = RandomizedSearchCV(GB_Classifier, param_grid_grad_boost, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_grad_boost.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 100, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_depth': 5, 'learning_rate': 0.5}\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.5, loss='deviance', max_depth=5,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=5, min_samples_split=4,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "gb_best_params_ = rand_search_grad_boost.best_params_\n",
    "gb_best_estimators_ = rand_search_grad_boost.best_estimator_\n",
    "\n",
    "print(gb_best_params_)\n",
    "print(gb_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8883677298311444"
      ]
     },
     "execution_count": 1412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_gb_estimator = gb_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_gb_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86625"
      ]
     },
     "execution_count": 1413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_gb_validation = gb_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_gb_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GB Classifier Model performs well on the validation set with 86.6% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  32 out of  32 | elapsed:   22.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [2, 10], 'min_samples_split': [2, 4], 'min_samples_leaf': [1, 2]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 1432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_classifier = ExtraTreesClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [2,10]\n",
    "min_samples_split = [2, 4]\n",
    "min_samples_leaf = [1, 2]  # Mhm, this one leads to accuracy of test and train sets being the same.\n",
    "\n",
    "param_grid_extra_trees = {'n_estimators' : n_estimators,\n",
    "                         'min_samples_split' : min_samples_split,\n",
    "                         'min_samples_leaf' : min_samples_leaf}\n",
    "\n",
    "\n",
    "rand_search_extra_trees = GridSearchCV(extra_classifier, param_grid_extra_trees, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_extra_trees.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 10}\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=2, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=42, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "et_best_params_ = rand_search_extra_trees.best_params_\n",
    "et_best_estimators_ = rand_search_extra_trees.best_estimator_\n",
    "\n",
    "print(et_best_params_)\n",
    "print(et_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8764853033145716"
      ]
     },
     "execution_count": 1434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_et_estimator = et_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_et_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8825"
      ]
     },
     "execution_count": 1435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_et_validation = et_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_et_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Extra Trees Classifier model performs well over the validation set with 88.25% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best'),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=10, n_jobs=1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 1451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bag_Classifier = BaggingClassifier(DecisionTreeClassifier(random_state=42))\n",
    "Bag_Classifier.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9480925578486554"
      ]
     },
     "execution_count": 1452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_bag_estimator = Bag_Classifier.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_bag_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86125"
      ]
     },
     "execution_count": 1453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_bag_estimator = Bag_Classifier.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y, y_pred_bag_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed:   17.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            ...n_estimators=10, n_jobs=1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [10, 12], 'max_samples': [0.4, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 1458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bag_Classifier = BaggingClassifier(DecisionTreeClassifier(random_state=42))\n",
    "\n",
    "n_estimators = [10, 12]\n",
    "max_samples = [.4, 1]\n",
    "\n",
    "param_grid_bag_clf = {'n_estimators':n_estimators, 'max_samples':max_samples}\n",
    "\n",
    "rand_search_bag_clf = GridSearchCV(Bag_Classifier, param_grid_bag_clf, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_bag_clf.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_samples': 0.4, 'n_estimators': 10}\n",
      "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
      "            splitter='best'),\n",
      "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
      "         max_samples=0.4, n_estimators=10, n_jobs=1, oob_score=False,\n",
      "         random_state=None, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "bag_best_params_ = rand_search_bag_clf.best_params_\n",
    "bag_best_estimators_ = rand_search_bag_clf.best_estimator_\n",
    "\n",
    "print(bag_best_params_)\n",
    "print(bag_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8918073796122576"
      ]
     },
     "execution_count": 1460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_bag_estimator = bag_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_bag_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88125"
      ]
     },
     "execution_count": 1461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_bag_validation = bag_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_bag_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bagging Classifier model performs well over the validation set with 88.12% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1615,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Architecture for Neural Networks.\n",
    "\n",
    "def reset_graph (seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Initialize the Input Layers and Hidden Layers\n",
    "n_inputs = 1000\n",
    "n_hidden1 = 10\n",
    "n_hidden2 = 150\n",
    "n_hidden3 = 70\n",
    "n_outputs = 2\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1616,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement dropout\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5 \n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training) \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, name=\"hidden1\", \n",
    "                              activation=tf.nn.relu)             \n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, name=\"hidden3\",\n",
    "                              activation=tf.nn.relu)\n",
    "\n",
    "    logit = tf.layers.dense(hidden3, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1617,
   "metadata": {},
   "outputs": [],
   "source": [
    "with  tf.name_scope (\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(tf.cast(y, tf.int32), depth = 2), logits=logit)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1618,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1619,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logit, tf.cast(y, tf.int64), 1) # tf.cast is new. \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1620,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define the directory to write the TensorBoard logs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1621,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1622,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = log_dir(\"TD_NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the FileWriter that we will use to write the TensorBoard logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1623,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1624,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_1, n_1 = x_train_tdm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1625,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X_train, y_train, batch_size):\n",
    "    rnd_indices = np.random.randint(0, len(X_train), batch_size)\n",
    "    X_batch = X_train[rnd_indices]\n",
    "    y_batch = y_train[rnd_indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining accuracy: 85.366% \tLoss: 0.68542\n",
      "Epoch: 5 \tTraining accuracy: 85.741% \tLoss: 0.64976\n",
      "Epoch: 10 \tTraining accuracy: 85.741% \tLoss: 0.61382\n",
      "Epoch: 15 \tTraining accuracy: 85.741% \tLoss: 0.58230\n",
      "Epoch: 20 \tTraining accuracy: 85.741% \tLoss: 0.55491\n",
      "Epoch: 25 \tTraining accuracy: 85.741% \tLoss: 0.53315\n",
      "Epoch: 30 \tTraining accuracy: 85.741% \tLoss: 0.51316\n",
      "Epoch: 35 \tTraining accuracy: 85.741% \tLoss: 0.49775\n",
      "Epoch: 40 \tTraining accuracy: 85.741% \tLoss: 0.48358\n",
      "Epoch: 45 \tTraining accuracy: 85.741% \tLoss: 0.47177\n",
      "Epoch: 50 \tTraining accuracy: 85.741% \tLoss: 0.46088\n",
      "Epoch: 55 \tTraining accuracy: 85.741% \tLoss: 0.45285\n",
      "Epoch: 60 \tTraining accuracy: 85.741% \tLoss: 0.44597\n",
      "Epoch: 65 \tTraining accuracy: 85.741% \tLoss: 0.44039\n",
      "Epoch: 70 \tTraining accuracy: 85.741% \tLoss: 0.43593\n",
      "Epoch: 75 \tTraining accuracy: 85.741% \tLoss: 0.43195\n",
      "Epoch: 80 \tTraining accuracy: 85.741% \tLoss: 0.42917\n",
      "Epoch: 85 \tTraining accuracy: 85.741% \tLoss: 0.42562\n",
      "Epoch: 90 \tTraining accuracy: 85.741% \tLoss: 0.42303\n",
      "Epoch: 95 \tTraining accuracy: 85.741% \tLoss: 0.42099\n",
      "Epoch: 100 \tTraining accuracy: 85.741% \tLoss: 0.41912\n",
      "Epoch: 105 \tTraining accuracy: 85.741% \tLoss: 0.41754\n",
      "Epoch: 110 \tTraining accuracy: 85.741% \tLoss: 0.41638\n",
      "Epoch: 115 \tTraining accuracy: 85.741% \tLoss: 0.41523\n",
      "Epoch: 120 \tTraining accuracy: 85.741% \tLoss: 0.41363\n",
      "Epoch: 125 \tTraining accuracy: 85.741% \tLoss: 0.41301\n",
      "Epoch: 130 \tTraining accuracy: 85.741% \tLoss: 0.41195\n",
      "Epoch: 135 \tTraining accuracy: 85.741% \tLoss: 0.41126\n",
      "Epoch: 140 \tTraining accuracy: 85.741% \tLoss: 0.41052\n",
      "Epoch: 145 \tTraining accuracy: 85.741% \tLoss: 0.41026\n",
      "Epoch: 150 \tTraining accuracy: 85.741% \tLoss: 0.40989\n",
      "Epoch: 155 \tTraining accuracy: 85.741% \tLoss: 0.40954\n",
      "Epoch: 160 \tTraining accuracy: 85.741% \tLoss: 0.40901\n",
      "Epoch: 165 \tTraining accuracy: 85.741% \tLoss: 0.40881\n",
      "Epoch: 170 \tTraining accuracy: 85.741% \tLoss: 0.40846\n",
      "Epoch: 175 \tTraining accuracy: 85.741% \tLoss: 0.40817\n",
      "Epoch: 180 \tTraining accuracy: 85.741% \tLoss: 0.40793\n",
      "Epoch: 185 \tTraining accuracy: 85.741% \tLoss: 0.40772\n",
      "Epoch: 190 \tTraining accuracy: 85.741% \tLoss: 0.40756\n",
      "Epoch: 195 \tTraining accuracy: 85.741% \tLoss: 0.40752\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "#Run the first model on the training set.\n",
    "\n",
    "n_epochs = 200\n",
    "batch_size = 15\n",
    "n_batches = int(np.ceil(m_1 / batch_size))\n",
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 50\n",
    "\n",
    "checkpoint_path = \"TD_NN/tmp/train_dnn_reg_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"TD_NN/train_dnn_reg_model\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "        sess.run(init_l)\n",
    "        for epoch in range(start_epoch, n_epochs):\n",
    "            for iteration in range(batch_size):\n",
    "                X_batch, y_batch = random_batch(x_train_tdm, np.array(train_set_y), batch_size)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})          \n",
    "            \n",
    "            accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss,accuracy_summary, loss_summary], \n",
    "                                                              feed_dict={X: x_train_tdm, y: np.array(train_set_y)})\n",
    "            \n",
    "            file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "            file_writer.add_summary(loss_summary_str, epoch)\n",
    "            if epoch % 5 == 0:\n",
    "                print(\"Epoch:\", epoch,\n",
    "                      \"\\tTraining accuracy: {:.3f}%\".format(accuracy_val * 100),\n",
    "                      \"\\tLoss: {:.5f}\".format(loss_val))\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "            if loss_val < best_loss:\n",
    "                saver.save(sess, final_model_path)\n",
    "                best_loss = loss_val\n",
    "                os.remove(checkpoint_epoch_path)\n",
    "            else:\n",
    "                epochs_without_progress += 5\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"Early stopping\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from TD_NN/train_dnn_reg_model\n",
      "0.8574109\n"
     ]
    }
   ],
   "source": [
    "#Get the Training accuracy\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    accuracy_value = accuracy.eval(feed_dict={X: x_train_tdm, y: train_set_y})\n",
    "\n",
    "print (accuracy_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from TD_NN/train_dnn_reg_model\n",
      "0.90125\n"
     ]
    }
   ],
   "source": [
    "#Get the validation accuracy\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    accuracy_value = accuracy.eval(feed_dict={X: x_val_tdm, y: validation_set_y})\n",
    "\n",
    "print (accuracy_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus Deep Nueral Network model is not well suited for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Voting Classifier - Ensemble the Models, that performed well on the validation models. These models are build on Random Forest, K-Nearest Neighbors, Extra Trees and Bagging Classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('nby_clf', MultinomialNB(alpha=1, class_prior=None, fit_prior=True)), ('log_clf', LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='libline...        presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
       "              warm_start=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 1568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_estimators= [ ('nby_clf', bayes_best_estimators_ ),\n",
    "                    ('log_clf', logreg_best_estimators_),\n",
    "                    ('gbo_clf', gb_best_estimators_)\n",
    "                  ]\n",
    "\n",
    "\n",
    "voting_clf  = VotingClassifier(total_estimators)\n",
    "\n",
    "voting_clf.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.892432770481551"
      ]
     },
     "execution_count": 1629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the y_pred to get accuracy score.\n",
    "y_pred_voting = voting_clf.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88875"
      ]
     },
     "execution_count": 1630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_voting_validation = voting_clf.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_voting_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1631,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_pred_voting_validation).to_csv(\"dnn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting Classifier doesn't impact much on the accuracy.\n",
    "\n",
    "###### So I consider the Logistic Regression model as my best model with 88.87% accuracy. The Evaluation Criteria for selecting the best model is ROC_AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1632,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now predict the accuracy with test Set\n",
    "\n",
    "y_pred_logreg_test = logreg_best_estimators_.predict(x_test_tdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1633,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test  = pd.DataFrame()\n",
    "result_test[\"Labels\"] = y_pred_logreg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1634,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test[\"Comment_no\"] = pd.DataFrame(list(range(4000,5572,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1635,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test.to_csv(\"Labels_logistic_regression.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
