{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries for NLTK\n",
    "from nltk.corpus import stopwords, gazetteers\n",
    "from nltk import sent_tokenize, wordpunct_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#Libraries for Text Processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Libraries for Evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Models\n",
    "from sklearn.ensemble import RandomForestClassifier     # Random Forest\n",
    "from sklearn.naive_bayes import MultinomialNB           # Nave Bayes\n",
    "from sklearn.linear_model import LogisticRegression     # logistic regression\n",
    "from sklearn.svm import SVC                             # Support Vector Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier      # K Nearest Neighbors\n",
    "from sklearn.tree import DecisionTreeClassifier         \n",
    "from sklearn.ensemble import AdaBoostClassifier         # Ada Boost Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier # Gradient Boosting\n",
    "from sklearn.ensemble import ExtraTreesClassifier       # Extra Trees Classifier\n",
    "from sklearn.ensemble import BaggingClassifier          # Bagging Classifier\n",
    "from sklearn.ensemble import VotingClassifier           # Ensemble Model\n",
    "import tensorflow as TF                                 # Deep Neural Networks\n",
    "\n",
    "# Other Libraries\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(123)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "# Cross-validation\n",
    "from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score #score evaluation\n",
    "from sklearn.model_selection import cross_val_predict #prediction\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_ROOT = \"C:\\\\Users\\\\Owner\\\\Desktop\\\\TD Text Analytics\\\\\"\n",
    "TD_SOURCE_FILE = FILE_ROOT + \"train.txt\"\n",
    "LABEL_FILE = FILE_ROOT + \"labels_Candidate.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this module only once - NLTK Stopwords \n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('gazetteers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing data - removing non ascii, links and entities like @, # and &. Further Tokenize by excluding Stop words & apply Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_non_ascii(string):\n",
    "    ''' Returns the string without non ASCII characters'''\n",
    "    stripped = (c for c in string if 0 < ord(c) < 127)\n",
    "    return ''.join(stripped)\n",
    "\n",
    "def strip_links(text):\n",
    "    link_regex = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], ', ')    \n",
    "    return text\n",
    "\n",
    "def strip_all_entities(text):\n",
    "    entity_prefixes = ['@','#', '&']\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,' ')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def pre_process(text):\n",
    "            \n",
    "        #Remove non-ascii characters\n",
    "        text = strip_non_ascii(text)\n",
    "        \n",
    "        #Strip @ # & entities\n",
    "        text = strip_all_entities(text)\n",
    "\n",
    "        #Strip URLs \n",
    "        text = strip_links(text)\n",
    "        \n",
    "        #Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        #Convert words with repeated characters\n",
    "        text = re.sub(r'([a-z])\\1{2,}',r'\\1',text)\n",
    "        \n",
    "        #Remove repeated words\n",
    "        text = re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1',text)\n",
    "    \n",
    "        #Replace #word with word\n",
    "        text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "\n",
    "        #Remove words with digits\n",
    "        text = re.sub(r'\\S*\\d\\S*','',text).strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = [word for sent in sent_tokenize(text) for word in wordpunct_tokenize(sent)]\n",
    "         \n",
    "    #Remove stopwords\n",
    "    stop = stopwords.words(\"english\")\n",
    "    \n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "\n",
    "    #Remove all words less than 2 chars\n",
    "    tokens = [token for token in tokens if len(token) >= 2]\n",
    "        \n",
    "    #Apply stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens  = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the Data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFile = pd.read_csv(TD_SOURCE_FILE, sep = None)\n",
    "\n",
    "DataFile.columns = [\"ID\", \"Wd\",\"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will merge all the words for every comment, to form complete sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewDataFile = DataFile.groupby('ID').apply(lambda x: x['Text'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will replace my lists to Strings for text analysis. This will also eliminate the brakets \"[]\" and the comma separator from the entire text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FormatDataFile = pd.DataFrame(NewDataFile.apply(lambda x:' '.join(x)))\n",
    "FormatDataFile.columns = [\"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Partition the data to identify the initial training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now split Back The data to training and test set - before applying the pipeline\n",
    "train_set, test_set = train_test_split(FormatDataFile, \n",
    "                                       test_size=0.28205,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3998, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1572, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can now add the labels as a new column to the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = pd.read_csv(LABEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3999, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the training labels\n",
    "Extract_Train_Set = Labels[0:3999]\n",
    "Extract_Train_Set.columns = [\"ID\", \"Label\"]\n",
    "Extract_Train_Set = Extract_Train_Set.drop(\"ID\",axis=1)\n",
    "\n",
    "# Get the first column for test set\n",
    "Extract_Text_Label = Labels[4000:]\n",
    "Extract_Text_Label.columns = [\"ID\", \"Label\"]\n",
    "Extract_Text_Label = Extract_Text_Label.drop(\"Label\",axis=1)\n",
    "\n",
    "Extract_Train_Set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['Label'] = Extract_Train_Set['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3998, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Training and Test Sets already defined and finalized, We also need a validation set to tune the model. Since we dont have test labels, Validation set approach will help improving over the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now split Back The training data to training and validation set - before applying the pipeline\n",
    "train_set, validation_set = train_test_split(train_set, \n",
    "                                       test_size=0.20,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define x and y.\n",
    "\n",
    "#the Y Variable\n",
    "train_set_y = train_set[\"Label\"].copy()\n",
    "validation_set_y = validation_set[\"Label\"].copy()\n",
    "\n",
    "#the X variables\n",
    "train_set_X = train_set.drop(\"Label\", axis=1)\n",
    "validation_set_X = validation_set.drop(\"Label\", axis=1)\n",
    "test_set_X = test_set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the TDM using the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer=\"word\", ngram_range=(1,2), preprocessor=pre_process, tokenizer=tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tdm = cv.fit_transform(train_set_X[\"Text\"])\n",
    "x_validation_tdm = cv.transform(validation_set_X[\"Text\"])\n",
    "x_test_tdm = cv.transform(test_set_X[\"Text\"])\n",
    "\n",
    "x_train_tdm = x_train_tdm.toarray()\n",
    "x_validation_tdm = x_validation_tdm.toarray()\n",
    "x_test_tdm = x_test_tdm.toarray()\n",
    "\n",
    "#Create the vocabulary and extract features in that Vocabulary.\n",
    "vocab = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3198, 30784)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tdm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Term Document Matrix has 30784 features. The next step is to reduce these dimensions and extract the best 1000 features using chi2 test statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch2 = SelectKBest(chi2, k=1000)\n",
    "x_train_tdm = ch2.fit_transform(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_tdm = ch2.transform(x_validation_tdm)\n",
    "x_test_tdm = ch2.transform(x_test_tdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the first model - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   39.9s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   40.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [50, 100, 400, 700, 1000], 'max_features': [5, 7, 10], 'max_depth': [10, 20], 'min_samples_split': [2, 4, 10, 12, 16], 'oob_score': [True, False], 'min_samples_leaf': [1, 5, 10], 'max_leaf_nodes': [2, 10, 20]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_class = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [50, 100, 400, 700, 1000]\n",
    "max_features = [5, 7, 10]\n",
    "max_depth = [10, 20] \n",
    "oob_score = [True, False]\n",
    "min_samples_split = [2, 4, 10, 12, 16]\n",
    "min_samples_leaf = [1, 5, 10] \n",
    "max_leaf_nodes = [2, 10, 20]\n",
    "\n",
    "\n",
    "param_grid_forest = {'n_estimators' : n_estimators, 'max_features' : max_features,\n",
    "                     'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n",
    "                    'oob_score' : oob_score, 'min_samples_leaf': min_samples_leaf, \n",
    "                     'max_leaf_nodes' : max_leaf_nodes}\n",
    "\n",
    "\n",
    "rand_search_forest = RandomizedSearchCV(forest_class, param_grid_forest, cv = 4, scoring='roc_auc', refit = True,\n",
    "                                 n_jobs = -1, verbose=2)\n",
    "\n",
    "rand_search_forest.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'oob_score': True, 'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_leaf_nodes': 20, 'max_features': 7, 'max_depth': 10}\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=10, max_features=7, max_leaf_nodes=20,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=10,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "            oob_score=True, random_state=42, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# Now we will try searching the best estimator and predict the values on the training set\n",
    "\n",
    "forest_best_params_ = rand_search_forest.best_params_\n",
    "forest_best_estimators_ = rand_search_forest.best_estimator_\n",
    "\n",
    "print(forest_best_params_)\n",
    "print(forest_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.868980612883052"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the training accuracy with Random Forest\n",
    "\n",
    "y_pred_forest = forest_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.855"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_forest_validation = forest_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_forest_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we notice, the accuracy has gone down in Random Forest from almost 87% to 85.5% in Validation Set. The model generalizes well but making a final conclusion that lets build more models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nave Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  32 out of  32 | elapsed:    6.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'alpha': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 1.5, 2]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_Bayes_ = MultinomialNB()\n",
    "\n",
    "alpha = [ 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 1.5, 2]\n",
    "\n",
    "param_grid_Bayes = {'alpha':alpha}\n",
    "\n",
    "grid_search_bayes = GridSearchCV(N_Bayes_, param_grid_Bayes, cv = 4, scoring='roc_auc', refit = True,\n",
    "                                 n_jobs = -1, verbose=2)\n",
    "\n",
    "grid_search_bayes.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1e-05}\n",
      "MultinomialNB(alpha=1e-05, class_prior=None, fit_prior=True)\n"
     ]
    }
   ],
   "source": [
    "bayes_best_params_ = grid_search_bayes.best_params_\n",
    "bayes_best_estimators_ = grid_search_bayes.best_estimator_\n",
    "\n",
    "print(bayes_best_params_)\n",
    "print(bayes_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9631019387116948"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the training accuracy with Naive Bayes\n",
    "\n",
    "y_pred_bayes = bayes_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7925"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_bayes_validation = bayes_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_bayes_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results shows a clear case of overfitting with Validation accuracy going down to 79.25%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    7.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'C': array([0.1, 0.2, ..., 9.8, 9.9])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state = 42)\n",
    "\n",
    "C = np.array(list(range(1, 100)))/10\n",
    "                            \n",
    "param_grid_log_reg = {'C' : C}\n",
    "\n",
    "rand_search_log_reg = RandomizedSearchCV(log_reg, param_grid_log_reg, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_log_reg.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.7}\n",
      "LogisticRegression(C=0.7, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "logreg_best_params_ = rand_search_log_reg.best_params_\n",
    "logreg_best_estimators_ = rand_search_log_reg.best_estimator_\n",
    "\n",
    "print(logreg_best_params_)\n",
    "print(logreg_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8927454659161976"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the training accuracy with Logistic Regression\n",
    "\n",
    "y_pred_logreg = logreg_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_logreg_validation = logreg_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_logreg_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Model accuracy drops from 89% to 85%, showing brief signs of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   53.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=42, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000002200ED57160>, 'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000002200EF7D160>},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC_Classifier = SVC(random_state = 42)\n",
    "\n",
    "param_distributions = {\"gamma\": reciprocal(0.0001, 0.001), \"C\": uniform(100000, 1000000)}\n",
    "\n",
    "rand_search_svc = RandomizedSearchCV(SVC_Classifier, param_distributions, n_iter=10, verbose=2, n_jobs = -1)\n",
    "\n",
    "rand_search_svc.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 672653.332911399, 'gamma': 0.0008941747769891772}\n",
      "SVC(C=672653.332911399, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.0008941747769891772,\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=42,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "svc_best_params_ = rand_search_svc.best_params_\n",
    "svc_best_estimators_ = rand_search_svc.best_estimator_\n",
    "\n",
    "print(svc_best_params_)\n",
    "print(svc_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.964665415884928"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_svc_estimator = svc_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_svc_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84125"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_svc_validation = svc_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_svc_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Model accuracy drops from 96% to 84%, showing clear signs of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K - Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 56 candidates, totalling 224 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 23.6min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 43.6min\n",
      "[Parallel(n_jobs=-1)]: Done 224 out of 224 | elapsed: 51.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_neighbors': [4, 6, 8, 10, 12, 14, 16, 18], 'leaf_size': [1, 3, 5, 7, 9, 11, 13]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Introduce KNN Classifier \n",
    "\n",
    "KNeighbours = KNeighborsClassifier()\n",
    "leaf_size = list(range(1,15,2))\n",
    "n_neighbors = list(range(4,20,2))\n",
    "\n",
    "param_grid_KNeighbours = {'n_neighbors' : n_neighbors,'leaf_size':leaf_size}\n",
    "\n",
    "grid_search_KNeighbours = GridSearchCV(KNeighbours, param_grid_KNeighbours, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "grid_search_KNeighbours.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'leaf_size': 3, 'n_neighbors': 6}\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=3, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=6, p=2,\n",
      "           weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "knn_best_params_ = grid_search_KNeighbours.best_params_\n",
    "knn_best_estimators_ = grid_search_KNeighbours.best_estimator_\n",
    "\n",
    "print(knn_best_params_)\n",
    "print(knn_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8692933083176986"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_knn_estimator = knn_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_knn_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.855"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_knn_validation = knn_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_knn_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN Classifier model performs well over the validation set with 85% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ADA Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:  8.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=42),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [50, 100, 400, 700, 1000], 'learning_rate': [0.001, 0.01, 0.05, 0.09], 'algorithm': ['SAMME', 'SAMME.R']},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_boost = AdaBoostClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [50, 100, 400, 700, 1000]\n",
    "learning_rate = [0.001, 0.01, 0.05, 0.09]\n",
    "algorithm = ['SAMME', 'SAMME.R']\n",
    "\n",
    "param_grid_ada = {'n_estimators' : n_estimators, 'learning_rate' : learning_rate, 'algorithm' : algorithm}\n",
    "\n",
    "rand_search_ada = RandomizedSearchCV(ada_boost, param_grid_ada, cv = 4, scoring='roc_auc', refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_ada.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1000, 'learning_rate': 0.05, 'algorithm': 'SAMME.R'}\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.05, n_estimators=1000, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "ada_best_params_ = rand_search_ada.best_params_\n",
    "ada_best_estimators_ = rand_search_ada.best_estimator_\n",
    "\n",
    "print(ada_best_params_)\n",
    "print(ada_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9036898061288305"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ada_estimator = ada_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_ada_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8425"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_ada_validation = ada_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_ada_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ADA Classifier Model accuracy drops from 90% to 84%, showing clear signs of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   57.4s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   59.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [50, 100, 400, 700, 1000], 'learning_rate': [0.1, 0.5], 'max_depth': [10, 20], 'min_samples_split': [2, 4, 10, 12, 16], 'min_samples_leaf': [1, 5, 10], 'max_features': [5, 20], 'max_leaf_nodes': [2, 10, 20]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB_Classifier = GradientBoostingClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [50, 100, 400, 700, 1000]\n",
    "learning_rate = [0.1, 0.5]\n",
    "max_depth = [10, 20]\n",
    "min_samples_split = [2, 4, 10, 12, 16]\n",
    "min_samples_leaf = [1, 5, 10]\n",
    "max_features = [5, 20]\n",
    "max_leaf_nodes = [2, 10, 20]\n",
    "                            \n",
    "param_grid_grad_boost = {'n_estimators' : n_estimators, 'learning_rate' : learning_rate,\n",
    "                              'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n",
    "                              'min_samples_leaf' : min_samples_leaf, 'max_features' : max_features,\n",
    "                              'max_leaf_nodes' : max_leaf_nodes}\n",
    "\n",
    "rand_search_grad_boost = RandomizedSearchCV(GB_Classifier, param_grid_grad_boost, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_grad_boost.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 700, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_leaf_nodes': 10, 'max_features': 20, 'max_depth': 10, 'learning_rate': 0.1}\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=10,\n",
      "              max_features=20, max_leaf_nodes=10,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=4,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=700,\n",
      "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "gb_best_params_ = rand_search_grad_boost.best_params_\n",
    "gb_best_estimators_ = rand_search_grad_boost.best_estimator_\n",
    "\n",
    "print(gb_best_params_)\n",
    "print(gb_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9643527204502814"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_gb_estimator = gb_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_gb_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83625"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_gb_validation = gb_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_gb_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GB Classifier Model accuracy drops from 96.4% to 83.6%, showing clear signs of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   23.1s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   33.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [50, 100, 400, 700, 1000], 'max_features': [5, 7, 10], 'max_depth': [10, 20], 'min_samples_split': [2, 4, 10, 12, 16], 'min_samples_leaf': [1, 5, 10]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_classifier = ExtraTreesClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [50, 100, 400, 700, 1000]\n",
    "max_features = [5, 7, 10]\n",
    "max_depth = [10, 20]\n",
    "min_samples_split = [2, 4, 10, 12, 16]\n",
    "min_samples_leaf = [1, 5, 10]  # Mhm, this one leads to accuracy of test and train sets being the same.\n",
    "\n",
    "param_grid_extra_trees = {'n_estimators' : n_estimators, 'max_features' : max_features,\n",
    "                         'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n",
    "                         'min_samples_leaf' : min_samples_leaf}\n",
    "\n",
    "\n",
    "rand_search_extra_trees = RandomizedSearchCV(extra_classifier, param_grid_extra_trees, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_extra_trees.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 700, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 5, 'max_depth': 10}\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=10, max_features=5, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=10,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=700, n_jobs=1,\n",
      "           oob_score=False, random_state=42, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "et_best_params_ = rand_search_extra_trees.best_params_\n",
    "et_best_estimators_ = rand_search_extra_trees.best_estimator_\n",
    "\n",
    "print(et_best_params_)\n",
    "print(et_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8696060037523452"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_et_estimator = et_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_et_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.855"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_et_validation = et_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_et_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Extra Trees Classifier model performs well over the validation set with 85% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            ...n_estimators=10, n_jobs=1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [50, 70, 100, 200, 500], 'max_samples': [10, 50, 100]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bag_Classifier = BaggingClassifier(DecisionTreeClassifier(random_state=42))\n",
    "\n",
    "n_estimators = [50,70,100,200,500]\n",
    "max_samples = [10,50,100]\n",
    "\n",
    "param_grid_bag_clf = {'n_estimators':n_estimators, 'max_samples':max_samples}\n",
    "\n",
    "rand_search_bag_clf = RandomizedSearchCV(Bag_Classifier, param_grid_bag_clf, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_bag_clf.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 500, 'max_samples': 50}\n",
      "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
      "            splitter='best'),\n",
      "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
      "         max_samples=50, n_estimators=500, n_jobs=1, oob_score=False,\n",
      "         random_state=None, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "bag_best_params_ = rand_search_bag_clf.best_params_\n",
    "bag_best_estimators_ = rand_search_bag_clf.best_estimator_\n",
    "\n",
    "print(bag_best_params_)\n",
    "print(bag_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.868980612883052"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_bag_estimator = bag_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_bag_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.855"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_bag_validation = bag_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_bag_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bagging Classifier model performs well over the validation set with 85% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Architecture for Neural Networks.\n",
    "\n",
    "def reset_graph (seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Initialize the Input Layers and Hidden Layers\n",
    "n_inputs = 1000\n",
    "n_hidden1 = 10\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 70\n",
    "n_outputs = 2\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement dropout\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5 \n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training) \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, name=\"hidden1\", \n",
    "                              activation=tf.nn.relu)             \n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, name=\"hidden3\",\n",
    "                              activation=tf.nn.relu)\n",
    "\n",
    "    logit = tf.layers.dense(hidden3, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with  tf.name_scope (\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(tf.cast(y, tf.int32), depth = 2), logits=logit)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logit, tf.cast(y, tf.int64), 1) # tf.cast is new. \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define the directory to write the TensorBoard logs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = log_dir(\"TD_NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the FileWriter that we will use to write the TensorBoard logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_1, n_1 = x_train_tdm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X_train, y_train, batch_size):\n",
    "    rnd_indices = np.random.randint(0, len(X_train), batch_size)\n",
    "    X_batch = X_train[rnd_indices]\n",
    "    y_batch = y_train[rnd_indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining accuracy: 82.333% \tLoss: 0.69183\n",
      "Epoch: 5 \tTraining accuracy: 86.773% \tLoss: 0.68417\n",
      "Epoch: 10 \tTraining accuracy: 87.054% \tLoss: 0.67672\n",
      "Epoch: 15 \tTraining accuracy: 86.929% \tLoss: 0.66941\n",
      "Epoch: 20 \tTraining accuracy: 86.929% \tLoss: 0.66229\n",
      "Epoch: 25 \tTraining accuracy: 86.961% \tLoss: 0.65538\n",
      "Epoch: 30 \tTraining accuracy: 86.929% \tLoss: 0.64847\n",
      "Epoch: 35 \tTraining accuracy: 86.929% \tLoss: 0.64202\n",
      "Epoch: 40 \tTraining accuracy: 86.929% \tLoss: 0.63547\n",
      "Epoch: 45 \tTraining accuracy: 86.929% \tLoss: 0.62927\n",
      "Epoch: 50 \tTraining accuracy: 86.929% \tLoss: 0.62313\n",
      "Epoch: 55 \tTraining accuracy: 86.929% \tLoss: 0.61711\n",
      "Epoch: 60 \tTraining accuracy: 86.898% \tLoss: 0.61146\n",
      "Epoch: 65 \tTraining accuracy: 86.898% \tLoss: 0.60587\n",
      "Epoch: 70 \tTraining accuracy: 86.898% \tLoss: 0.60035\n",
      "Epoch: 75 \tTraining accuracy: 86.898% \tLoss: 0.59501\n",
      "Epoch: 80 \tTraining accuracy: 86.898% \tLoss: 0.58977\n",
      "Epoch: 85 \tTraining accuracy: 86.898% \tLoss: 0.58451\n",
      "Epoch: 90 \tTraining accuracy: 86.898% \tLoss: 0.57977\n",
      "Epoch: 95 \tTraining accuracy: 86.898% \tLoss: 0.57511\n",
      "Epoch: 100 \tTraining accuracy: 86.898% \tLoss: 0.57061\n",
      "Epoch: 105 \tTraining accuracy: 86.898% \tLoss: 0.56610\n",
      "Epoch: 110 \tTraining accuracy: 86.898% \tLoss: 0.56169\n",
      "Epoch: 115 \tTraining accuracy: 86.898% \tLoss: 0.55731\n",
      "Epoch: 120 \tTraining accuracy: 86.898% \tLoss: 0.55319\n",
      "Epoch: 125 \tTraining accuracy: 86.898% \tLoss: 0.54905\n",
      "Epoch: 130 \tTraining accuracy: 86.898% \tLoss: 0.54501\n",
      "Epoch: 135 \tTraining accuracy: 86.898% \tLoss: 0.54113\n",
      "Epoch: 140 \tTraining accuracy: 86.898% \tLoss: 0.53746\n",
      "Epoch: 145 \tTraining accuracy: 86.898% \tLoss: 0.53397\n",
      "Epoch: 150 \tTraining accuracy: 86.898% \tLoss: 0.53037\n",
      "Epoch: 155 \tTraining accuracy: 86.898% \tLoss: 0.52704\n",
      "Epoch: 160 \tTraining accuracy: 86.898% \tLoss: 0.52365\n",
      "Epoch: 165 \tTraining accuracy: 86.898% \tLoss: 0.52032\n",
      "Epoch: 170 \tTraining accuracy: 86.898% \tLoss: 0.51706\n",
      "Epoch: 175 \tTraining accuracy: 86.898% \tLoss: 0.51386\n",
      "Epoch: 180 \tTraining accuracy: 86.898% \tLoss: 0.51083\n",
      "Epoch: 185 \tTraining accuracy: 86.898% \tLoss: 0.50782\n",
      "Epoch: 190 \tTraining accuracy: 86.898% \tLoss: 0.50480\n",
      "Epoch: 195 \tTraining accuracy: 86.898% \tLoss: 0.50201\n"
     ]
    }
   ],
   "source": [
    "#Run the first model on the training set.\n",
    "\n",
    "n_epochs = 200\n",
    "batch_size = 25\n",
    "n_batches = int(np.ceil(m_1 / batch_size))\n",
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 50\n",
    "\n",
    "checkpoint_path = \"TD_NN/tmp/train_dnn_reg_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"TD_NN/train_dnn_reg_model\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "        sess.run(init_l)\n",
    "        for epoch in range(start_epoch, n_epochs):\n",
    "            for iteration in range(batch_size):\n",
    "                X_batch, y_batch = random_batch(x_train_tdm, np.array(train_set_y), batch_size)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})          \n",
    "            \n",
    "            accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss,accuracy_summary, loss_summary], \n",
    "                                                              feed_dict={X: x_train_tdm, y: np.array(train_set_y)})\n",
    "            \n",
    "            file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "            file_writer.add_summary(loss_summary_str, epoch)\n",
    "            if epoch % 5 == 0:\n",
    "                print(\"Epoch:\", epoch,\n",
    "                      \"\\tTraining accuracy: {:.3f}%\".format(accuracy_val * 100),\n",
    "                      \"\\tLoss: {:.5f}\".format(loss_val))\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "            if loss_val < best_loss:\n",
    "                saver.save(sess, final_model_path)\n",
    "                best_loss = loss_val\n",
    "                os.remove(checkpoint_epoch_path)\n",
    "            else:\n",
    "                epochs_without_progress += 5\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"Early stopping\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from TD_NN/train_dnn_reg_model\n",
      "0.8689806\n"
     ]
    }
   ],
   "source": [
    "#Get the Training accuracy\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    accuracy_value = accuracy.eval(feed_dict={X: x_train_tdm, y: train_set_y})\n",
    "\n",
    "print (accuracy_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from TD_NN/train_dnn_reg_model\n",
      "0.855\n"
     ]
    }
   ],
   "source": [
    "#Get the validation accuracy\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    accuracy_value = accuracy.eval(feed_dict={X: x_val_tdm, y: validation_set_y})\n",
    "\n",
    "print (accuracy_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Voting Classifier - Ensemble the Models, which have performed well on the validation models. These models are build on Random Forest, K-Nearest Neighbors, Extra Trees and Bagging Classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rfo_clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features=7, max_leaf_nodes=20,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=10,\n",
       "            min_weigh...stimators=500, n_jobs=1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_estimators= [ ('rfo_clf', forest_best_estimators_), \n",
    "                    ('knn_clf', knn_best_estimators_),\n",
    "                    ('etc_clf', et_best_estimators_),\n",
    "                    ('bag_clf', bag_best_estimators_)\n",
    "                  ]\n",
    "\n",
    "\n",
    "voting_clf  = VotingClassifier(total_estimators)\n",
    "\n",
    "voting_clf.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.868980612883052"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the y_pred to get accuracy score.\n",
    "y_pred_voting = voting_clf.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.855"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_voting_validation = voting_clf.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_voting_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting Classifier doesn't impact much on the accuracy.\n",
    "\n",
    "###### So I consider the Voting Classifier as my best model with 85.5% accuracy. The Evaluation Criteria for selecting the best model is ROC_AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now predict the accuracy with test Set\n",
    "y_pred_voting_test = voting_clf.predict(x_test_tdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_pred_voting_test).to_csv(\"new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = pd.DataFrame()\n",
    "comment_id_test = Extract_Text_Label[\"ID\"].copy()\n",
    "result_test[\"Comment No.\"] = comment_id_test\n",
    "result_test[\"Label\"] = pd.DataFrame(y_pred_voting_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test.to_csv(\"results_voting.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
