{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries for NLTK\n",
    "from nltk.corpus import stopwords, gazetteers\n",
    "from nltk import sent_tokenize, wordpunct_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#Libraries for Text Processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# Libraries for Evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "#Models\n",
    "from sklearn.ensemble import RandomForestClassifier     # Random Forest\n",
    "from sklearn.naive_bayes import MultinomialNB           # Nave Bayes\n",
    "from sklearn.linear_model import LogisticRegression     # logistic regression\n",
    "from sklearn.svm import SVC                             # Support Vector Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier      # K Nearest Neighbors\n",
    "from sklearn.tree import DecisionTreeClassifier         \n",
    "from sklearn.ensemble import AdaBoostClassifier         # Ada Boost Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier # Gradient Boosting\n",
    "from sklearn.ensemble import ExtraTreesClassifier       # Extra Trees Classifier\n",
    "from sklearn.ensemble import BaggingClassifier          # Bagging Classifier\n",
    "from sklearn.ensemble import VotingClassifier           # Ensemble Model\n",
    "import tensorflow as TF                                 # Deep Neural Networks\n",
    "\n",
    "# Other Libraries\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(123)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "# Cross-validation\n",
    "from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score #score evaluation\n",
    "from sklearn.model_selection import cross_val_predict #prediction\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_ROOT = \"C:\\\\Users\\\\Owner\\\\Desktop\\\\TD Text Analytics\\\\\"\n",
    "TD_SOURCE_FILE = FILE_ROOT + \"train.txt\"\n",
    "LABEL_FILE = FILE_ROOT + \"labels_Candidate.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this module only once - NLTK Stopwords \n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('gazetteers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing data - removing non ascii, links and entities like @, # and &. Further Tokenize by excluding Stop words & apply Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_non_ascii(string):\n",
    "    ''' Returns the string without non ASCII characters'''\n",
    "    stripped = (c for c in string if 0 < ord(c) < 127)\n",
    "    return ''.join(stripped)\n",
    "\n",
    "def pre_process(text):\n",
    "            \n",
    "        #Remove non-ascii characters\n",
    "        text = strip_non_ascii(text)\n",
    "                \n",
    "        #Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        #Convert words with repeated characters\n",
    "        text = re.sub(r'([a-z])\\1{2,}',r'\\1',text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = [word for sent in sent_tokenize(text) for word in wordpunct_tokenize(sent)]\n",
    "         \n",
    "    #Remove stopwords\n",
    "    stop = stopwords.words(\"english\")\n",
    "    \n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "\n",
    "    #Remove all words less than 1 character\n",
    "    tokens = [token for token in tokens if len(token) >= 1]\n",
    "        \n",
    "    #Apply stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens  = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the Data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFile = pd.read_csv(TD_SOURCE_FILE, sep = None)\n",
    "\n",
    "DataFile.columns = [\"ID\", \"Wd\",\"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will merge all the words for every comment, to form complete sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewDataFile = DataFile.groupby('ID').apply(lambda x: x['Text'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will replace my lists to Strings for text analysis. This will also eliminate the brakets \"[]\" and the comma separator from the entire text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "FormatDataFile = pd.DataFrame(NewDataFile.apply(lambda x:' '.join(x)))\n",
    "FormatDataFile.columns = [\"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Partition the data to identify the initial training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now split Back The data to training and test set - before applying the pipeline\n",
    "train_set, test_set = train_test_split(FormatDataFile, \n",
    "                                       test_size=0.28205,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3998, 1)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1572, 1)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can now add the labels as a new column to the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = pd.read_csv(LABEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3999, 1)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the training labels\n",
    "Extract_Train_Set = Labels[0:3999]\n",
    "Extract_Train_Set.columns = [\"ID\", \"Label\"]\n",
    "Extract_Train_Set = Extract_Train_Set.drop(\"ID\",axis=1)\n",
    "\n",
    "# Get the first column for test set\n",
    "Extract_Text_Label = Labels[4000:]\n",
    "Extract_Text_Label.columns = [\"ID\", \"Label\"]\n",
    "Extract_Text_Label = Extract_Text_Label.drop(\"Label\",axis=1)\n",
    "\n",
    "Extract_Train_Set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['Label'] = Extract_Train_Set['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3998, 2)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3998, 2)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Training and Test Sets already defined and finalized, We also need a validation set to tune the model. Since we dont have test labels, Validation set approach will help improving over the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now split Back The training data to training and validation set - before applying the pipeline\n",
    "training_set, validation_set = train_test_split(train_set, \n",
    "                                       test_size=0.20,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define x and y.\n",
    "\n",
    "#the Y Variable\n",
    "train_set_y = training_set[\"Label\"].copy()\n",
    "validation_set_y = validation_set[\"Label\"].copy()\n",
    "\n",
    "#the X variables\n",
    "train_set_X = training_set.drop(\"Label\", axis=1)\n",
    "validation_set_X = validation_set.drop(\"Label\", axis=1)\n",
    "test_set_X = test_set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the TDM using the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer=\"word\", ngram_range=(1,2), preprocessor=pre_process, tokenizer=tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tdm = cv.fit_transform(train_set_X[\"Text\"])\n",
    "x_validation_tdm = cv.transform(validation_set_X[\"Text\"])\n",
    "x_test_tdm = cv.transform(test_set_X[\"Text\"])\n",
    "\n",
    "x_train_tdm = x_train_tdm.toarray()\n",
    "x_validation_tdm = x_validation_tdm.toarray()\n",
    "x_test_tdm = x_test_tdm.toarray()\n",
    "\n",
    "#Create the vocabulary and extract features in that Vocabulary.\n",
    "vocab = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3198, 33886)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tdm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Term Document Matrix has 33886 features. The next step is to reduce these dimensions and extract the best 100 features using f_classif test statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch2 = SelectKBest(f_classif, k=100)\n",
    "x_train_tdm = ch2.fit_transform(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_tdm = ch2.transform(x_validation_tdm)\n",
    "x_test_tdm = ch2.transform(x_test_tdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the first model - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   19.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [50, 100, 400, 700, 1000], 'max_features': [5, 7, 10], 'max_depth': [10, 20], 'min_samples_split': [2, 4, 10, 12, 16], 'oob_score': [True, False], 'min_samples_leaf': [1, 5, 10], 'max_leaf_nodes': [2, 10, 20]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_class = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [50, 100, 400, 700, 1000]\n",
    "max_features = [5, 7, 10]\n",
    "max_depth = [10, 20] \n",
    "oob_score = [True, False]\n",
    "min_samples_split = [2, 4, 10, 12, 16]\n",
    "min_samples_leaf = [1, 5, 10] \n",
    "max_leaf_nodes = [2, 10, 20]\n",
    "\n",
    "\n",
    "param_grid_forest = {'n_estimators' : n_estimators, 'max_features' : max_features,\n",
    "                     'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n",
    "                    'oob_score' : oob_score, 'min_samples_leaf': min_samples_leaf, \n",
    "                     'max_leaf_nodes' : max_leaf_nodes}\n",
    "\n",
    "\n",
    "rand_search_forest = RandomizedSearchCV(forest_class, param_grid_forest, cv = 4, scoring='roc_auc', refit = True,\n",
    "                                 n_jobs = -1, verbose=2)\n",
    "\n",
    "rand_search_forest.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'oob_score': True, 'n_estimators': 1000, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_leaf_nodes': 2, 'max_features': 5, 'max_depth': 20}\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=20, max_features=5, max_leaf_nodes=2,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=4,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
      "            oob_score=True, random_state=42, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# Now we will try searching the best estimator and predict the values on the training set\n",
    "\n",
    "forest_best_params_ = rand_search_forest.best_params_\n",
    "forest_best_estimators_ = rand_search_forest.best_estimator_\n",
    "\n",
    "print(forest_best_params_)\n",
    "print(forest_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.868980612883052"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the training accuracy with Random Forest\n",
    "\n",
    "y_pred_forest = forest_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.855"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_forest_validation = forest_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_forest_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we notice, Random Forest has performed well on the validation set with 85.5% accuracy. The model generalizes well but making a final conclusion that lets build more models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nave Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  32 out of  32 | elapsed:    3.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'alpha': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 1.5, 2]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_Bayes_ = MultinomialNB()\n",
    "\n",
    "alpha = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 1.5, 2]\n",
    "\n",
    "param_grid_Bayes = {'alpha':alpha}\n",
    "\n",
    "grid_search_bayes = GridSearchCV(N_Bayes_, param_grid_Bayes, cv = 4, scoring='roc_auc', refit = True,\n",
    "                                 n_jobs = -1, verbose=2)\n",
    "\n",
    "grid_search_bayes.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.001}\n",
      "MultinomialNB(alpha=0.001, class_prior=None, fit_prior=True)\n"
     ]
    }
   ],
   "source": [
    "bayes_best_params_ = grid_search_bayes.best_params_\n",
    "bayes_best_estimators_ = grid_search_bayes.best_estimator_\n",
    "\n",
    "print(bayes_best_params_)\n",
    "print(bayes_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.890556597873671"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the training accuracy with Naive Bayes\n",
    "\n",
    "y_pred_bayes = bayes_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.835"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_bayes_validation = bayes_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_bayes_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results shows a good case of overfitting with Validation accuracy going down to 83.37%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    4.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'C': array([0.1, 0.2, ..., 9.8, 9.9])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state = 42)\n",
    "\n",
    "C = np.array(list(range(1, 100)))/10\n",
    "                            \n",
    "param_grid_log_reg = {'C' : C}\n",
    "\n",
    "rand_search_log_reg = RandomizedSearchCV(log_reg, param_grid_log_reg, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_log_reg.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.7}\n",
      "LogisticRegression(C=0.7, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "logreg_best_params_ = rand_search_log_reg.best_params_\n",
    "logreg_best_estimators_ = rand_search_log_reg.best_estimator_\n",
    "\n",
    "print(logreg_best_params_)\n",
    "print(logreg_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8749218261413383"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the training accuracy with Logistic Regression\n",
    "\n",
    "y_pred_logreg = logreg_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85375"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_logreg_validation = logreg_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_logreg_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model performs well on the validation set with 85.37% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    9.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=42, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000178E8552BA8>, 'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000178E86D1048>},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC_Classifier = SVC(random_state = 42)\n",
    "\n",
    "param_distributions = {\"gamma\": reciprocal(0.0001, 0.001), \"C\": uniform(100000, 1000000)}\n",
    "\n",
    "rand_search_svc = RandomizedSearchCV(SVC_Classifier, param_distributions, n_iter=10, verbose=2, n_jobs = -1)\n",
    "\n",
    "rand_search_svc.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 500752.9320304509, 'gamma': 0.0008216275767080257}\n",
      "SVC(C=500752.9320304509, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.0008216275767080257,\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=42,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "svc_best_params_ = rand_search_svc.best_params_\n",
    "svc_best_estimators_ = rand_search_svc.best_estimator_\n",
    "\n",
    "print(svc_best_params_)\n",
    "print(svc_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8949343339587242"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_svc_estimator = svc_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_svc_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83875"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_svc_validation = svc_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_svc_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Model accuracy drops from 89.4% to 83.8%, showing clear signs of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K - Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 56 candidates, totalling 224 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 224 out of 224 | elapsed:  7.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_neighbors': [4, 6, 8, 10, 12, 14, 16, 18], 'leaf_size': [1, 3, 5, 7, 9, 11, 13]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Introduce KNN Classifier \n",
    "\n",
    "KNeighbours = KNeighborsClassifier()\n",
    "leaf_size = list(range(1,15,2))\n",
    "n_neighbors = list(range(4,20,2))\n",
    "\n",
    "param_grid_KNeighbours = {'n_neighbors' : n_neighbors,'leaf_size':leaf_size}\n",
    "\n",
    "grid_search_KNeighbours = GridSearchCV(KNeighbours, param_grid_KNeighbours, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "grid_search_KNeighbours.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'leaf_size': 1, 'n_neighbors': 12}\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=1, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=12, p=2,\n",
      "           weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "knn_best_params_ = grid_search_KNeighbours.best_params_\n",
    "knn_best_estimators_ = grid_search_KNeighbours.best_estimator_\n",
    "\n",
    "print(knn_best_params_)\n",
    "print(knn_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8692933083176986"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_knn_estimator = knn_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_knn_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.855"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_knn_validation = knn_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_knn_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN Classifier model performs well over the validation set with 85% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ADA Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   50.0s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   57.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=42),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [50, 100, 400, 700, 1000], 'learning_rate': [0.001, 0.01, 0.05, 0.09], 'algorithm': ['SAMME', 'SAMME.R']},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_boost = AdaBoostClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [50, 100, 400, 700, 1000]\n",
    "learning_rate = [0.001, 0.01, 0.05, 0.09]\n",
    "algorithm = ['SAMME', 'SAMME.R']\n",
    "\n",
    "param_grid_ada = {'n_estimators' : n_estimators, 'learning_rate' : learning_rate, 'algorithm' : algorithm}\n",
    "\n",
    "rand_search_ada = RandomizedSearchCV(ada_boost, param_grid_ada, cv = 4, scoring='roc_auc', refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_ada.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1000, 'learning_rate': 0.05, 'algorithm': 'SAMME.R'}\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.05, n_estimators=1000, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "ada_best_params_ = rand_search_ada.best_params_\n",
    "ada_best_estimators_ = rand_search_ada.best_estimator_\n",
    "\n",
    "print(ada_best_params_)\n",
    "print(ada_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8946216385240775"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ada_estimator = ada_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_ada_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.835"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_ada_validation = ada_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_ada_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ADA Classifier Model accuracy drops from 89.4% to 83.5%, showing clear signs of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   30.7s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   32.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [50, 100, 400, 700, 1000], 'learning_rate': [0.1, 0.5], 'max_depth': [10, 20], 'min_samples_split': [2, 4, 10, 12, 16], 'min_samples_leaf': [1, 5, 10], 'max_features': [5, 20], 'max_leaf_nodes': [2, 10, 20]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB_Classifier = GradientBoostingClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [50, 100, 400, 700, 1000]\n",
    "learning_rate = [0.1, 0.5]\n",
    "max_depth = [10, 20]\n",
    "min_samples_split = [2, 4, 10, 12, 16]\n",
    "min_samples_leaf = [1, 5, 10]\n",
    "max_features = [5, 20]\n",
    "max_leaf_nodes = [2, 10, 20]\n",
    "                            \n",
    "param_grid_grad_boost = {'n_estimators' : n_estimators, 'learning_rate' : learning_rate,\n",
    "                              'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n",
    "                              'min_samples_leaf' : min_samples_leaf, 'max_features' : max_features,\n",
    "                              'max_leaf_nodes' : max_leaf_nodes}\n",
    "\n",
    "rand_search_grad_boost = RandomizedSearchCV(GB_Classifier, param_grid_grad_boost, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_grad_boost.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_leaf_nodes': 10, 'max_features': 20, 'max_depth': 10, 'learning_rate': 0.1}\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=10,\n",
      "              max_features=20, max_leaf_nodes=10,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=10,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "gb_best_params_ = rand_search_grad_boost.best_params_\n",
    "gb_best_estimators_ = rand_search_grad_boost.best_estimator_\n",
    "\n",
    "print(gb_best_params_)\n",
    "print(gb_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8921200750469043"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_gb_estimator = gb_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_gb_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83625"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_gb_validation = gb_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_gb_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GB Classifier Model accuracy drops from 89.2% to 83.6%, showing brief signs of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   24.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [50, 100, 400, 700, 1000], 'max_features': [5, 7, 10], 'max_depth': [10, 20], 'min_samples_split': [2, 4, 10, 12, 16], 'min_samples_leaf': [1, 5, 10]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_classifier = ExtraTreesClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [50, 100, 400, 700, 1000]\n",
    "max_features = [5, 7, 10]\n",
    "max_depth = [10, 20]\n",
    "min_samples_split = [2, 4, 10, 12, 16]\n",
    "min_samples_leaf = [1, 5, 10]  # Mhm, this one leads to accuracy of test and train sets being the same.\n",
    "\n",
    "param_grid_extra_trees = {'n_estimators' : n_estimators, 'max_features' : max_features,\n",
    "                         'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n",
    "                         'min_samples_leaf' : min_samples_leaf}\n",
    "\n",
    "\n",
    "rand_search_extra_trees = RandomizedSearchCV(extra_classifier, param_grid_extra_trees, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_extra_trees.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 700, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 5, 'max_depth': 10}\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=10, max_features=5, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=10,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=700, n_jobs=1,\n",
      "           oob_score=False, random_state=42, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "et_best_params_ = rand_search_extra_trees.best_params_\n",
    "et_best_estimators_ = rand_search_extra_trees.best_estimator_\n",
    "\n",
    "print(et_best_params_)\n",
    "print(et_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8721075672295184"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_et_estimator = et_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_et_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.855"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_et_validation = et_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_et_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Extra Trees Classifier model performs well over the validation set with 85% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   31.1s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   34.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            ...n_estimators=10, n_jobs=1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [50, 70, 100, 200, 500], 'max_samples': [10, 50, 100]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bag_Classifier = BaggingClassifier(DecisionTreeClassifier(random_state=42))\n",
    "\n",
    "n_estimators = [50,70,100,200,500]\n",
    "max_samples = [10,50,100]\n",
    "\n",
    "param_grid_bag_clf = {'n_estimators':n_estimators, 'max_samples':max_samples}\n",
    "\n",
    "rand_search_bag_clf = RandomizedSearchCV(Bag_Classifier, param_grid_bag_clf, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_bag_clf.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 500, 'max_samples': 50}\n",
      "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
      "            splitter='best'),\n",
      "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
      "         max_samples=50, n_estimators=500, n_jobs=1, oob_score=False,\n",
      "         random_state=None, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "bag_best_params_ = rand_search_bag_clf.best_params_\n",
    "bag_best_estimators_ = rand_search_bag_clf.best_estimator_\n",
    "\n",
    "print(bag_best_params_)\n",
    "print(bag_best_estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.868980612883052"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_bag_estimator = bag_best_estimators_.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_bag_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.855"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_bag_validation = bag_best_estimators_.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_bag_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bagging Classifier model performs well over the validation set with 85% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Architecture for Neural Networks.\n",
    "\n",
    "def reset_graph (seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Initialize the Input Layers and Hidden Layers\n",
    "n_inputs = 100\n",
    "n_hidden1 = 10\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 70\n",
    "n_outputs = 2\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement dropout\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5 \n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training) \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, name=\"hidden1\", \n",
    "                              activation=tf.nn.relu)             \n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, name=\"hidden3\",\n",
    "                              activation=tf.nn.relu)\n",
    "\n",
    "    logit = tf.layers.dense(hidden3, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "with  tf.name_scope (\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(tf.cast(y, tf.int32), depth = 2), logits=logit)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logit, tf.cast(y, tf.int64), 1) # tf.cast is new. \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define the directory to write the TensorBoard logs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = log_dir(\"TD_NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the FileWriter that we will use to write the TensorBoard logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_1, n_1 = x_train_tdm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X_train, y_train, batch_size):\n",
    "    rnd_indices = np.random.randint(0, len(X_train), batch_size)\n",
    "    X_batch = X_train[rnd_indices]\n",
    "    y_batch = y_train[rnd_indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining accuracy: 87.523% \tLoss: 0.69177\n",
      "Epoch: 5 \tTraining accuracy: 87.367% \tLoss: 0.68419\n",
      "Epoch: 10 \tTraining accuracy: 87.086% \tLoss: 0.67690\n",
      "Epoch: 15 \tTraining accuracy: 87.117% \tLoss: 0.66978\n",
      "Epoch: 20 \tTraining accuracy: 87.054% \tLoss: 0.66286\n",
      "Epoch: 25 \tTraining accuracy: 86.961% \tLoss: 0.65617\n",
      "Epoch: 30 \tTraining accuracy: 86.992% \tLoss: 0.64948\n",
      "Epoch: 35 \tTraining accuracy: 86.961% \tLoss: 0.64321\n",
      "Epoch: 40 \tTraining accuracy: 86.929% \tLoss: 0.63686\n",
      "Epoch: 45 \tTraining accuracy: 86.929% \tLoss: 0.63081\n",
      "Epoch: 50 \tTraining accuracy: 86.929% \tLoss: 0.62488\n",
      "Epoch: 55 \tTraining accuracy: 86.929% \tLoss: 0.61904\n",
      "Epoch: 60 \tTraining accuracy: 86.929% \tLoss: 0.61357\n",
      "Epoch: 65 \tTraining accuracy: 86.929% \tLoss: 0.60815\n",
      "Epoch: 70 \tTraining accuracy: 86.898% \tLoss: 0.60282\n",
      "Epoch: 75 \tTraining accuracy: 86.898% \tLoss: 0.59763\n",
      "Epoch: 80 \tTraining accuracy: 86.898% \tLoss: 0.59254\n",
      "Epoch: 85 \tTraining accuracy: 86.898% \tLoss: 0.58742\n",
      "Epoch: 90 \tTraining accuracy: 86.898% \tLoss: 0.58280\n",
      "Epoch: 95 \tTraining accuracy: 86.898% \tLoss: 0.57823\n",
      "Epoch: 100 \tTraining accuracy: 86.898% \tLoss: 0.57382\n",
      "Epoch: 105 \tTraining accuracy: 86.898% \tLoss: 0.56941\n",
      "Epoch: 110 \tTraining accuracy: 86.898% \tLoss: 0.56508\n",
      "Epoch: 115 \tTraining accuracy: 86.898% \tLoss: 0.56078\n",
      "Epoch: 120 \tTraining accuracy: 86.898% \tLoss: 0.55672\n",
      "Epoch: 125 \tTraining accuracy: 86.898% \tLoss: 0.55266\n",
      "Epoch: 130 \tTraining accuracy: 86.898% \tLoss: 0.54869\n",
      "Epoch: 135 \tTraining accuracy: 86.898% \tLoss: 0.54487\n",
      "Epoch: 140 \tTraining accuracy: 86.898% \tLoss: 0.54125\n",
      "Epoch: 145 \tTraining accuracy: 86.898% \tLoss: 0.53780\n",
      "Epoch: 150 \tTraining accuracy: 86.898% \tLoss: 0.53424\n",
      "Epoch: 155 \tTraining accuracy: 86.898% \tLoss: 0.53095\n",
      "Epoch: 160 \tTraining accuracy: 86.898% \tLoss: 0.52759\n",
      "Epoch: 165 \tTraining accuracy: 86.898% \tLoss: 0.52429\n",
      "Epoch: 170 \tTraining accuracy: 86.898% \tLoss: 0.52106\n",
      "Epoch: 175 \tTraining accuracy: 86.898% \tLoss: 0.51788\n",
      "Epoch: 180 \tTraining accuracy: 86.898% \tLoss: 0.51486\n",
      "Epoch: 185 \tTraining accuracy: 86.898% \tLoss: 0.51188\n",
      "Epoch: 190 \tTraining accuracy: 86.898% \tLoss: 0.50886\n",
      "Epoch: 195 \tTraining accuracy: 86.898% \tLoss: 0.50608\n"
     ]
    }
   ],
   "source": [
    "#Run the first model on the training set.\n",
    "\n",
    "n_epochs = 200\n",
    "batch_size = 25\n",
    "n_batches = int(np.ceil(m_1 / batch_size))\n",
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 50\n",
    "\n",
    "checkpoint_path = \"TD_NN/tmp/train_dnn_reg_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"TD_NN/train_dnn_reg_model\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "        sess.run(init_l)\n",
    "        for epoch in range(start_epoch, n_epochs):\n",
    "            for iteration in range(batch_size):\n",
    "                X_batch, y_batch = random_batch(x_train_tdm, np.array(train_set_y), batch_size)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})          \n",
    "            \n",
    "            accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss,accuracy_summary, loss_summary], \n",
    "                                                              feed_dict={X: x_train_tdm, y: np.array(train_set_y)})\n",
    "            \n",
    "            file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "            file_writer.add_summary(loss_summary_str, epoch)\n",
    "            if epoch % 5 == 0:\n",
    "                print(\"Epoch:\", epoch,\n",
    "                      \"\\tTraining accuracy: {:.3f}%\".format(accuracy_val * 100),\n",
    "                      \"\\tLoss: {:.5f}\".format(loss_val))\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "            if loss_val < best_loss:\n",
    "                saver.save(sess, final_model_path)\n",
    "                best_loss = loss_val\n",
    "                os.remove(checkpoint_epoch_path)\n",
    "            else:\n",
    "                epochs_without_progress += 5\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"Early stopping\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from TD_NN/train_dnn_reg_model\n",
      "0.8689806\n"
     ]
    }
   ],
   "source": [
    "#Get the Training accuracy\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    accuracy_value = accuracy.eval(feed_dict={X: x_train_tdm, y: train_set_y})\n",
    "\n",
    "print (accuracy_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from TD_NN/train_dnn_reg_model\n",
      "0.855\n"
     ]
    }
   ],
   "source": [
    "#Get the validation accuracy\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    accuracy_value = accuracy.eval(feed_dict={X: x_val_tdm, y: validation_set_y})\n",
    "\n",
    "print (accuracy_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Voting Classifier - Ensemble the Models, which have performed well on the validation models. These models are build on Random Forest, K-Nearest Neighbors, Extra Trees and Bagging Classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rfo_clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=20, max_features=5, max_leaf_nodes=2,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=4,\n",
       "            min_weight_...stimators=500, n_jobs=1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_estimators= [ ('rfo_clf', forest_best_estimators_), \n",
    "                    ('knn_clf', knn_best_estimators_),\n",
    "                    ('etc_clf', et_best_estimators_),\n",
    "                    ('bag_clf', bag_best_estimators_)\n",
    "                  ]\n",
    "\n",
    "\n",
    "voting_clf  = VotingClassifier(total_estimators)\n",
    "\n",
    "voting_clf.fit(x_train_tdm, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.868980612883052"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the y_pred to get accuracy score.\n",
    "y_pred_voting = voting_clf.predict(x_train_tdm)\n",
    "accuracy_score(train_set_y, y_pred_voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.855"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now predict the accuracy with validation Set\n",
    "\n",
    "y_pred_voting_validation = voting_clf.predict(x_val_tdm)\n",
    "accuracy_score(validation_set_y,y_pred_voting_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting Classifier doesn't impact much on the accuracy.\n",
    "\n",
    "###### So I consider Deep Neural Networks as my best model with 85.5% accuracy. The Evaluation Criteria for selecting the best model is ROC_AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from TD_NN/train_dnn_reg_model\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    dnn_test_output = sess.run(tf.argmax(logit,1),feed_dict={X: x_test_tdm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test  = pd.DataFrame()\n",
    "result_test[\"Labels\"] = dnn_test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test[\"Comment_no\"] = pd.DataFrame(list(range(4000,5572,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test.to_csv(\"output_labels.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
